{
  "draft": "content/drafts/postgres-at-scale.md",
  "check_date": "2025-11-09T13:33:52.842831",
  "total_claims": 26,
  "verified": 0,
  "unverified": 26,
  "failed": 0,
  "details": [
    {
      "claim": "## Read Replicas: The 80% Solution to \"Scale\" Problems\n\nMost applications are read-heavy.",
      "type": "percentage",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "E-commerce platforms: 90% reads.",
      "type": "percentage",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "Social networks: 85%+ reads.",
      "type": "percentage",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "SaaS applications: 80%+ reads.",
      "type": "percentage",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "## Query Optimization: Where Most \"Scale\" Problems Actually Live\n\nHere's the uncomfortable truth: 80% of Postgres \"scale\" problems are actually query problems masquerading as infrastructure problems.",
      "type": "percentage",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "**Common index strategies**:\n- **B-tree indexes**: Standard index for equality and range queries (90% of use cases)\n- **Partial indexes**: Index only rows matching condition (`WHERE deleted_at IS NULL`) - saves space, faster\n- **Covering indexes**: Include additional columns in index (`INCLUDE (name, email)`) - avoid table lookups\n- **GIN indexes**: Full-text search, JSONB queries, array operations\n- **BRIN indexes**: Large tables with natural ordering (time-series data, append-only logs)\n\nReal-world example from production:\n- Table: 50 million rows\n- Query: User dashboard aggregation\n- Before indexing: 8-second query\n- After proper indexes: 120ms query\n- Cost: 10 minutes to create index vs 6 months to migrate database\n\n## Operational Reality: Backups, Upgrades, and Monitoring\n\nScaling Postgres isn't just query throughput\u2014it's operational maturity.",
      "type": "percentage",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "(Target >99% cache hits)\n\nThe teams running Postgres successfully at scale aren't the ones with the fanciest distributed architecture.",
      "type": "percentage",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "For 80% of applications, Postgres remains the right choice at any scale.",
      "type": "percentage",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "**Read replicas solve 80% of throughput problems.",
      "type": "percentage",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "Distributed databases solve real problems for the <5% of applications that truly need them.",
      "type": "percentage",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "The consulting proposal on your desk estimates $500,000 and six months of engineering time for the migration.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "A database struggling with 100TB of data might have no problem with 50,000 QPS.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "A system choking on 10,000 concurrent connections might handle writes beautifully with proper pooling.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "16xlarge (512GB RAM, 64 vCPUs): ~$6,500/month\n- Distributed database migration (engineering time, new infrastructure, operational complexity): $500,000+ and 6-12 months\n- Years of runway before vertical scaling caps out: 2-4 years for typical growth trajectories\n\nThe math is straightforward: if you can solve your problem by upgrading instance size, you save half a million dollars and avoid re-architecting your application.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "**Primary-replica architecture** is Postgres's bread and butter:\n- One primary database (handles all writes and consistent reads)\n- Multiple read replicas (handle read-only queries)\n- Streaming replication keeps replicas in sync (sub-second lag typical)\n\nThis pattern scales you from 10,000 to 100,000+ queries per second without changing your application architecture significantly.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "**Real-world pattern**:\n- Primary: Write operations, critical reads requiring strong consistency\n- Replica 1: User-facing dashboard queries\n- Replica 2: Analytics and reporting queries\n- Replica 3: Background job queries\n\nEach replica can handle 10,000-50,000 QPS depending on query complexity.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "That's 30,000-150,000 QPS total throughput from a four-server setup.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "At 1,000+ concurrent connections, you're spending gigabytes of RAM just on connection management.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "**The problem**:\n- Each Postgres connection: ~10MB of memory overhead\n- 5,000 concurrent connections: 50GB RAM gone before any queries run\n- Connection establishment: 5-10ms overhead per connection\n- Modern applications (microservices, serverless): thousands of ephemeral connections\n\n**The solution**: Connection pooling with PgBouncer or Pgpool-II\n\nPgBouncer sits between your application and Postgres, maintaining a pool of persistent database connections and multiplexing application requests across them.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "**Typical improvement**:\n- Before pooling: 5,000 application connections \u2192 5,000 Postgres connections (database crashes)\n- After pooling: 5,000 application connections \u2192 100 Postgres connections (database purrs)\n\nConnection pooling often solves \"scale\" problems that look like throughput issues but are actually connection management failures.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "User ID 1-1,000,000 go to Shard 1.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "User ID 1,000,001-2,000,000 go to Shard 2.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "**When you actually need sharding**:\n- Your largest Postgres instance can't hold your data working set\n- Write throughput exceeds what a single primary can handle (rare\u2014Postgres handles 10,000+ writes/sec on modern hardware)\n- You've exhausted vertical scaling and read replicas\n- Multi-tenancy with strict tenant isolation requirements\n\n**The complexity cost nobody tells you about**:\n- **Cross-shard queries**: Want to query across all users?",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "**Fix**:\n```sql\nCREATE INDEX idx_users_email ON users(email);\n-- Refactor query to not use leading wildcard, or use full-text search\n```\n\nNew execution time: 12ms instead of 45,000ms.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "## When to Actually Consider Distributed Databases\n\nDespite everything above, distributed databases solve real problems for real use cases:\n\n**Geographic distribution with low latency**:\n- Users in US, Europe, Asia need <50ms latency\n- Data residency requirements (GDPR, localization laws)\n- Multi-region active-active writes\n\n**True horizontal write scaling**:\n- You've exhausted vertical scaling (128 vCPUs, 768GB RAM not enough)\n- Write throughput exceeds 50,000 writes/second\n- Sharding complexity is worth the write scaling gain\n\n**Specialized workloads**:\n- Time-series data at extreme scale (consider TimescaleDB, InfluxDB)\n- Graph queries (consider Neo4j, DGraph)\n- Document-heavy workloads (consider MongoDB for schemaless)\n\n**Distributed SQL options**:\n- **CockroachDB**: Postgres-compatible, automatic sharding, multi-region, strong consistency\n- **YugabyteDB**: Postgres-compatible, based on PostgreSQL code, Raft consensus\n- **AWS Aurora**: Cloud-native Postgres with storage layer distributed across AZs\n\nThese tools are engineering marvels.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    },
    {
      "claim": "** PgBouncer should be in your stack before you hit 1,000 concurrent connections.",
      "type": "statistic",
      "verification": {
        "verified": null,
        "confidence": 0.5,
        "source": null,
        "note": "Unverified - proceed with caution"
      }
    }
  ]
}